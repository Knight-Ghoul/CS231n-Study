[Neural Networks Part 2: Setting up the Data and the Loss](http://cs231n.github.io/neural-networks-2/)
===
[toc]
#1. 设置数据和模型
- 概念：Course Note5中介绍了**神经元模型** ，它在计算内积后进行非线性的激活函数计算，而**神经网络**将这些神经元组织成各个层。这些做法共同定义了评分函数（score function）的新形式。具体来说，**神经网络**就是进行了一系列的线性映射与非线性激活函数交织的运算。这节课讨论更多的算法设计选项。如数据预处理、权重初始化和损失函数等。
##数据预处理
- **均值减法（Mean subtraction）**
对数据中每个独立特征减去平均值，可以理解为将数据云的中心迁移到原点。X -= np.mean(X)
- **归一化（Normalization）**
是指将数据的所有维度都归一化，使其数值范围都近似相等。先对数据做零中心化/均值减法（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。(只有在feature具有相似的标准和度量方式时才有意义)

![](https://pic1.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.png)
一般数据预处理流程：
**左边**：原始的2维输入数据。
**中间**：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。
**右边**：每个维度都除以其标准差来调整其数值范围。
红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。

---
- PCA和白化(Whitening)(一般不用)
先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。

![](https://pic3.zhimg.com/80/aae11de6e6a29f50d46b9ea106fbb02a_hd.png)
PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。

---
- **常见错误**。进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。**应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。**

## 权重初始化
1. 全零初始化 **[X]**[错误]
 所有神经元输出相同、梯度相同、反向传播中以相同形式更新，得到完全相同的神经元。
2. **小随机数初始化**
 实现方法是：W = 0.01 * np.random.randn(D,H)。其中randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。（在小的神经网络上可以，但用在更深的网络上会出问题）
3. **使用1/sqrt(n)校准方差**
 上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。通过除以输入数据量的平方根来调整其数值范围，使得神经元输出的方差归一化到1。这里是为了保证，经过linear function后的输出方差大小近似，而不是正比于输入的数据个数。
 常用方法是除以sqrt(n)，其中n是输入数据的数量。实践经验证明，这样做可以提高收敛的速度。
 目前常用的方法是Kaiming法，将随机方差设为 $\sqrt{2.0/n}$，即：
```python
w = np.random.randn(n) * sqrt(2.0/n)
```
4. 稀疏初始化(这里有一些问题):仅将很少数目的权重做高斯初始化，其余均设为0
5. 偏置(biases)初始化:bias全部初始化为0
- **实践**。当前的推荐是使用ReLU激活函数，并且使用w = np.random.randn(n) * sqrt(2.0/n)来进行权重初始化
##批量归一化(batch normalization)：
其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。
    - 在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层.
    - 目前在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。
    - 总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起
##正则化(L2/L1/Maxnorm/Dropout)
1. L1/L2 regularization：前面已经提过两次了：L1是sparse，L2是spread
    - L2:可直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。
    - L1:让权重向量在最优化的过程中变得稀疏（即非常接近0）

2. 最大范式约束（Max norm constraints）
给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。
3.随机失活（Dropout）
与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数[公式]的概率被激活或者被设置为0。
#损失函数
不同的问题有不同的代价函数，对于classfication而言，常用的代价函数是SVM和softmax。
具体问题具体讨论。